{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Using callback to act on a model during training\n",
    "\n",
    "1. When you’re training a model, there are many things you can’t predict from the start. In particular, you can’t tell how many epochs\n",
    "   will be needed to get to an optimal validation loss. The examples so far have adopted the strategy of training for enough epochs that you\n",
    "   begin overfitting, using the first run to figure out the proper number of epochs to train for, and then finally launching a new training\n",
    "   run from scratch using this optimal number. Of course, this approach is wasteful.\n",
    "\n",
    "2. A much better way to handle this is to stop training when you measure that the validation loss in no longer improving. This can be\n",
    "   achieved using a Keras callback. A callback is an object (a class instance implementing specific methods) that is passed to the model\n",
    "   in the call to fit and that is called by the model at various points during training. It has access to all the available data about\n",
    "   the state of the model and its performance, and it can take action: interrupt training, save a model, load a different weight set, or\n",
    "   otherwise alter the state of the model.\n",
    "\n",
    "3. Some examples of ways you can use callbacks:\n",
    "   (*) Model checkpointing — Saving the current weights of the model at different points during training.\n",
    "   (*) Early stopping — Interrupting training when the validation loss is no longer improving (and of course, saving the best model\n",
    "                         obtained during training).\n",
    "   (*) Dynamically adjusting the value of certain parameters during training—Such as the learning rate of the optimizer.\n",
    "   (*) Logging training and validation metrics during training, or visualizing the representations learned by the model as they’re updated \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The model checkpoint & early stopping callbacks\n",
    "\n",
    "1. You can use the EarlyStopping callback to interrupt training once a target metric being monitored has stopped improving\n",
    "   for a fixed number of epochs.\n",
    "\n",
    "2. This callback is typically used in combination with ModelCheckpoint, which lets you continually save the model during training\n",
    "   and, optionally, save only the current best model so far.\n",
    "\"\"\"\n",
    "\n",
    "# Interrupts training when accuracy has stopped improving for more than one epoch \n",
    "# Saves the current weights after every epoch, and won’t overwrite the model file unless val_loss has improve\n",
    "callbacks_list = [ keras.callbacks.EarlyStopping(monitor='acc', patience=1,),\n",
    "                   keras.callbacks.ModelCheckpoint(filepath='my_model.h5', monitor='val_loss', save_best_only=True,)]\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "model.fit(x, y,\n",
    "          epochs=10,\n",
    "          batch_size=32,\n",
    "          callbacks=callbacks_list,\n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The reduce LR on plateau callback\n",
    "\n",
    "You can use this callback to reduce the learning rate when the validation loss has stopped improving. Reducing or increasing the learning\n",
    "rate in case of a loss plateau is an effective strategy to get out of local minima during training.\n",
    "\"\"\"\n",
    "\n",
    "# Divides the learning rate by 10 when triggered\n",
    "# The callback is triggered after the validation loss has stopped improving for 10 epochs\n",
    "callbacks_list = [ keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10,)]\n",
    "\n",
    "model.fit(x, y,\n",
    "          epochs=10,\n",
    "          batch_size=32,\n",
    "          callbacks=callbacks_list,\n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write your own callback\n",
    "\n",
    "1. If you need to take a specific action during training that isn’t covered by one of the built-in callbacks, you can write your own\n",
    "   callback. Callbacks are implemented by subclassing the class keras.callbacks.Callback. You can then implement any number of the following\n",
    "   transparently named methods, which are called at various points during training:\n",
    "   -- on_epoch_begin\n",
    "   -- on_epoch_end\n",
    "   -- on_batch_begin\n",
    "   -- on_batch_end\n",
    "   -- on_train_begin\n",
    "   -- on_train_end\n",
    "\n",
    "2. These methods all are called with a logs argument, which is a dictionary containing information about the previous batch, epoch,\n",
    "   or training run: training and validation metrics, and so on. Additionally, the callback has access to the following attributes:\n",
    "   (*) self.model — The model instance from which the callback is being called\n",
    "   (*) self.validation_data — The value of what was passed to fit as validation data\n",
    "\"\"\"\n",
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "class ActivationLogger(keras.callbacks.Callback):\n",
    "    def set_model(self, model):\n",
    "        self.model = model\n",
    "        layer_outputs = [layer.output for layer in model.layers]\n",
    "        self.activations_model = keras.models.Model(model.input, layer_outputs)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if self.validation_data is None:\n",
    "            raise RuntimeError('Requires validation_data.')\n",
    "        validation_sample = self.validation_data[0][0:1]\n",
    "        activations = self.activations_model.predict(validation_sample)\n",
    "        f = open('activations_at_epoch_' + str(epoch) + '.npz', 'w')\n",
    "        np.savez(f, activations)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TensorBoard\n",
    "\n",
    "1. To do good research or develop good models, you need rich, frequent feedback about what’s going on inside your models during your\n",
    "   experiments. That’s the point of running experiments: to get information about how well a model performs—as much information as\n",
    "   possible. Making progress is an iterative process, or loop: you start with an idea and express it as an experiment, attempting to\n",
    "   validate or invalidate your idea. You run this experiment and process the information it generates. This inspires your next idea.\n",
    "   The more iterations of this loop you’re able to run, the more refined and powerful your ideas become. Keras helps you go from idea to\n",
    "   experiment in the least possible time, and fast GPUs can help you get from experiment to result as quickly as possible.\n",
    "\n",
    "2.  TensorBoard is a browser-based visualization tool that comes packaged with TensorFlow. The key purpose of TensorBoard is to help you\n",
    "    visually monitor everything that goes on inside your model during training. If you’re monitoring more information than just the\n",
    "    model’s final loss, you can develop a clearer vision of what the model does and doesn’t do, and you can make progress more quickly:\n",
    "    (*) Visually monitoring metrics during training\n",
    "    (*) Visualizing your model architecture\n",
    "    (*) Visualizing histograms of activations and gradients\n",
    "    (*) Exploring embeddings in 3D\n",
    "\"\"\"\n",
    "# Before you start using TensorBoard, you need to create a directory where you’ll store the log files it generates.\n",
    "#!mkdir tf_borad_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Text-classification model to use with TensorBoard\n",
    "\"\"\"\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "import numpy as np\n",
    "\n",
    "max_features = 10000\n",
    "max_len = 500\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_len)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_len)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(layers.Embedding(max_features, 128, input_length=max_len))\n",
    "model.add(layers.Conv1D(32, 7, activation='relu')) # 32: number of filters, 7: kernal size\n",
    "model.add(layers.MaxPooling1D(5))\n",
    "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.compile(optimizer=RMSprop(lr=1e-4), loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "# Tensorflow callback: Records activation histograms every 1 epoch & embedding data every 1 epoch\n",
    "callbacks = [ callbacks.TensorBoard(log_dir='./tf_borad_log',\n",
    "                                    histogram_freq=1,)]\n",
    "                                    #embeddings_layer_names='embed')] ???\n",
    "\n",
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=128,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=callbacks,\n",
    "                    verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils  import plot_model\n",
    "\n",
    "plot_model(model, to_file='model.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
