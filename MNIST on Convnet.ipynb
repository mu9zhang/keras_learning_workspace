{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 3, 3, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                36928     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 93,322\n",
      "Trainable params: 93,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "\n",
    "# A basic convnet\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1))) # input_shape = (height, width, channel)\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "model.add(layers.Flatten()) # flatten the 3D outputs to 1D\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.0163 - acc: 0.9951\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.0131 - acc: 0.9961\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 3s 48us/step - loss: 0.0106 - acc: 0.9965\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.0092 - acc: 0.9972\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.0075 - acc: 0.9977\n",
      "10000/10000 [==============================] - 0s 36us/step\n",
      "0.03589198187187212 0.9923\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# load the data\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "train_images = train_images.astype('float') / 255\n",
    "\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "test_images = test_images.astype('float') / 255\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels  = to_categorical(test_labels)\n",
    "\n",
    "# train the model\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(train_images, train_labels, epochs=5, batch_size=64)\n",
    "\n",
    "# evaluate the model\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(test_loss, test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The Convolution Operation\n",
    "\n",
    "1. The fundamental difference between a densely connected layer and a convolution layer is this: Dense layers learn global patterns\n",
    "   in their input feature space (for example, for a MNIST digit, patterns involving all pixels), whereas convolution layers learn local\n",
    "   patterns: in the case of images, patterns found in small 2D windows of the inputs. In the previous example, these windows were all 3 × 3.\n",
    "\n",
    "2. Two interesting properties of convnet:\n",
    "   -- The patterns they learn are translation invariant. After learning a certain pattern in the lower-right corner of a picture,\n",
    "      a convnet can recognize it anywhere: for example, in the upper-left corner. A densely connected network would have to learn\n",
    "      the pattern anew if it appeared at a new location. This makes convnets data efficient when processing images (because the visual\n",
    "      world is fundamentally translation invariant): they need fewer training samples to learn representations that have generalization\n",
    "      power.\n",
    "   -- They can learn spatial hierarchies of patterns. A first convolution layer will learn small local patterns such as edges, a second\n",
    "      convolution layer will learn larger patterns made of the features of the first layers, and so on. This allows convnets to efficiently\n",
    "      learn increasingly complex and abstract visual concepts (because the visual world is fundamentally spatially hierarchical).\n",
    "\n",
    "3. Convolutions operate over 3D tensors, called feature maps, with two spatial axes (height and width) as well as a depth axis (also called\n",
    "   the channels axis). The convolution operation extracts patches from its input feature map and applies the same transformation to all of\n",
    "   these patches, producing an output feature map. This output feature map is still a 3D tensor: it has a width and a height. Its depth\n",
    "   can be arbitrary, because the output depth is a parameter of the layer, and the different channels in that depth axis no longer stand for\n",
    "   specific colors as in RGB input; rather, they stand for filters. Filters encode specific aspects of the input data: at a high level,\n",
    "   a single filter could encode the concept “presence of a face in the input,” for instance.\n",
    "\n",
    "4.  In the MNIST example, the first convolution layer takes a feature map of size (28, 28, 1) and outputs a feature map of size (26, 26, 32):\n",
    "    it computes 32 filters over its input. Each of these 32 output channels contains a 26 × 26 grid of values, which is a response map of\n",
    "    the filter over the input, indicating the response of that filter pattern at different locations in the input。\n",
    "\n",
    "5. Convolutions are defined by two key parameters:\n",
    "   -- Size of the patches extracted from the inputs—These are typically 3 × 3 or 5 × 5.\n",
    "   -- Depth of the output feature map—The number of filters computed by the convolution.\n",
    "\n",
    "6. In Conv2D layers, padding is configurable via the padding argument, which takes two values: \"valid\", which means no padding\n",
    "   (only valid window locations will be used); and \"same\", which means “pad in such a way as to have an output with the same width\n",
    "   and height as the input.” The padding argument defaults to \"valid\".\n",
    "   \n",
    "7. The distance between two successive windows is a parameter of the convolution, called its stride, which defaults to 1. It’s possible\n",
    "   to have strided convolutions: convolutions with a stride higher than 1. Using stride 2 means the width and height of the feature map\n",
    "   are downsampled by a factor of 2 (in addition to any changes induced by border effects). To downsample feature maps, instead of strides,\n",
    "   we tend to use the max-pooling operation.\n",
    "\n",
    "8.  Max pooling consists of extracting windows from the input feature maps and outputting the max value of each channel. It’s conceptually\n",
    "    similar to convolution, except that instead of transforming local patches via a learned linear transformation (the convolution kernel),\n",
    "    they’re transformed via a hardcoded max tensor operation. A big difference from convolution is that max pooling is usually done\n",
    "    with 2 × 2 windows and stride 2, in order to downsample the feature maps by a factor of 2. On the other hand, convolution is typically\n",
    "    done with 3 × 3 windows and no stride (stride 1).\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 22, 22, 64)        36928     \n",
      "=================================================================\n",
      "Total params: 55,744\n",
      "Trainable params: 55,744\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Why max-pooling?\n",
    "\n",
    "1. It isn’t conducive to learning a spatial hierarchy of features. The 3 × 3 windows in the third layer will only contain information\n",
    "   coming from 7 × 7 windows in the initial input. The high-level patterns learned by the convnet will still be very small with regard to\n",
    "   the initial input, which may not be enough to learn to classify digits (try recognizing a digit by only looking at it through windows\n",
    "   that are 7 × 7 pixels!). We need the features from the last convolution layer to contain information about the totality of the input.\n",
    "\n",
    "2. The final feature map has 22 × 22 × 64 = 30,976 total coefficients per sample. This is huge. If you were to flatten it to stick\n",
    "   a Dense layer of size 512 on top, that layer would have 15.8 million parameters. This is far too large for such a small model and would\n",
    "   result in intense overfitting.\n",
    "\n",
    "3. In short, the reason to use downsampling is to reduce the number of feature-map coefficients to process, as well as to induce\n",
    "   spatial-filter hierarchies by making successive convolution layers look at increasingly large windows.\n",
    "\n",
    "4. Note that max pooling isn’t the only way you can achieve such downsampling. As you already know, you can also use strides in the prior\n",
    "   convolution layer. And you can use average pooling instead of max pooling, where each local input patch is transformed by taking\n",
    "   the average value of each channel over the patch, rather than the max. But max pooling tends to work better than these alternative\n",
    "   solutions. In a nutshell, the reason is that features tend to encode the spatial presence of some pattern or concept over the different\n",
    "   tiles of the feature map, and it’s more informative to look at the maximal presence of different features than at their average presence.\n",
    "   So the most reasonable subsampling strategy is to first produce dense maps of features (via unstrided convolutions) and then look\n",
    "   at the maximal activation of the features over small patches, rather than looking at sparser windows of the inputs\n",
    "   (via strided convolutions) or averaging input patches, which could cause you to miss or dilute feature-presence information.\n",
    "\"\"\"\n",
    "\n",
    "model_no_max_pool = models.Sequential()\n",
    "\n",
    "model_no_max_pool.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model_no_max_pool.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model_no_max_pool.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "print(model_no_max_pool.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
