{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Beyond the Sequential model:\n",
    "\n",
    "1. Some networks require several independent inputs, others require multiple outputs, and some networks have internal branching between\n",
    "   layers that makes them look like graphs of layers rather than linear stacks of layers.\n",
    "\n",
    "2. Some tasks, for instance, require multimodal inputs: they merge data coming from different input sources, processing each type of data\n",
    "   using different kinds of neural layers. Imagine a deep-learning model trying to predict the most likely market price of a second-hand\n",
    "   piece of clothing, using the following inputs: user-provided metadata (such as the item’s brand, age, and so on), a user-provided text\n",
    "   description, and a picture of the item. If you had only the metadata available, you could one-hot encode it and use a densely connected\n",
    "   network to predict the price. If you had only the text description available, you could use an RNN or a 1D convnet. If you had only\n",
    "   the picture, you could use a 2D convnet. But how can you use all three at the same time? A naive approach would be to train three\n",
    "   separate models and then do a weighted average of their predictions. But this may be suboptimal, because the information extracted by\n",
    "   the models may be redundant. A better way is to jointly learn a more accurate model of the data by using a model that can see all\n",
    "   available input modalities simultaneously: a model with three input branches.\n",
    "\n",
    "3. Similarly, some tasks need to predict multiple target attributes of input data. Given the text of a novel or short story, you might want\n",
    "   to automatically classify it by genre (such as romance or thriller) but also predict the approximate date it was written. Of course,\n",
    "   you could train two separate models: one for the genre and one for the date. But because these attributes aren’t statistically\n",
    "   independent, you could build a better model by learning to jointly predict both genre and date at the same time. Such a joint model would\n",
    "   then have two outputs, or heads. Due to correlations between genre and date, knowing the date of a novel would help the model learn rich,\n",
    "   accurate representations of the space of novel genres, and vice versa.\n",
    "\n",
    "4. Additionally, many recently developed neural architectures require nonlinear network topology: networks structured as directed acyclic\n",
    "   graphs. The Inception family of networks for instance, relies on Inception modules, where the input is processed by several parallel\n",
    "   convolutional branches whose outputs are then merged back into a single tensor. There’s also the recent trend of adding residual\n",
    "   connections to a model, which started with the ResNet family of networks. A residual connection consists of reinjecting previous\n",
    "   representations into the downstream flow of data by adding a past output tensor to a later output tensor, which helps prevent information\n",
    "   loss along the data-processing flow.\n",
    "\n",
    "5. These three important use cases — multi-input models, multi-output models, and graph-like models — aren’t possible when using only\n",
    "   the Sequential model class in Keras. But there’s another far more general and flexible way to use Keras: the functional API. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 3,466\n",
      "Trainable params: 3,466\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "seq_mode:  None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 3,466\n",
      "Trainable params: 3,466\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "function api:  None\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Functional API\n",
    "\n",
    "1. In the functional API, you directly manipulate tensors, and you use layers as functions that take tensors and return tensors\n",
    "\n",
    "2. The only part that may seem a bit magical at this point is instantiating a Model object using only an input tensor and an output tensor.\n",
    "   Behind the scenes, Keras retrieves every layer involved in going from input_tensor to output_tensor, bringing them together into\n",
    "   a graph-like data structure—a Model. Of course, the reason it works is that output_tensor was obtained by repeatedly transforming\n",
    "   input_tensor.\n",
    "\n",
    "3. When it comes to compiling, training, or evaluating such an instance of Model, the API is the same as that of Sequential:\n",
    "\"\"\"\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Input\n",
    "\n",
    "# sequence model\n",
    "seq_model = Sequential()\n",
    "seq_model.add(layers.Dense(32, activation='relu', input_shape=(64,)))\n",
    "seq_model.add(layers.Dense(32, activation='relu'))\n",
    "seq_model.add(layers.Dense(10, activation='softmax'))\n",
    "print(\"seq_mode: \", seq_model.summary())\n",
    "\n",
    "# function api\n",
    "input_tensor = Input(shape=(64,))\n",
    "x = layers.Dense(32, activation='relu')(input_tensor)\n",
    "x = layers.Dense(32, activation='relu')(x)\n",
    "output_tensor = layers.Dense(10, activation='softmax')(x)\n",
    "model = Model(input_tensor, output_tensor)\n",
    "print(\"function api: \", model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "text (InputLayer)               (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "question (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, None, 10000)  640000      text[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, None, 10000)  320000      question[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   (None, 32)           1284224     embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                   (None, 16)           641088      embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 48)           0           lstm_4[0][0]                     \n",
      "                                                                 lstm_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 500)          24500       concatenate_2[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 2,909,812\n",
      "Trainable params: 2,909,812\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Multi-input Models\n",
    "\n",
    "1. The functional API can be used to build models that have multiple inputs. Typically, such models at some point merge their different input\n",
    "   branches using a layer that can combine several tensors: by adding them, concatenating them, and so on. This is usually done via a Keras\n",
    "   merge operation such as keras.layers.add, keras.layers.concatenate, and so on.\n",
    "\n",
    "2. A typical question-answering model has two inputs: a natural-language question and a text snippet (such as a news article) providing\n",
    "   information to be used for answering the question. The model must then produce an answer: in the simplest possible setup, this is a\n",
    "   one-word answer obtained via a softmax over some predefined vocabulary.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "text_vocabulary_size = 10000\n",
    "question_vocabulary_size = 10000\n",
    "answer_vocabulary_size = 500\n",
    "\n",
    "# embed the text input text\n",
    "text_input = Input(shape=(None,), dtype='int32', name='text')\n",
    "embedded_text = layers.Embedding(64, text_vocabulary_size)(text_input)\n",
    "encoded_text = layers.LSTM(32)(embedded_text)\n",
    "\n",
    "# embed the question input text\n",
    "question_input = Input(shape=(None,), dtype='int32', name='question')\n",
    "embedded_question = layers.Embedding(32, question_vocabulary_size)(question_input)\n",
    "encoded_question = layers.LSTM(16)(embedded_question)\n",
    "\n",
    "# concatenates the encoded question and encoded text, and then do the softmax\n",
    "concatenated = layers.concatenate([encoded_text, encoded_question],axis=-1)\n",
    "answer = layers.Dense(answer_vocabulary_size, activation='softmax')(concatenated)\n",
    "\n",
    "# specify the two inputs and one ouput at model instantiation\n",
    "model = Model([text_input, question_input], answer)\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "\"\"\"\n",
    "# feeding data to a multi-input model\n",
    "num_samples = 1000\n",
    "max_length = 100\n",
    "\n",
    "text     = np.random.randint(1, text_vocabulary_size, size=(num_samples, max_length))\n",
    "question = np.random.randint(1, question_vocabulary_size, size=(num_samples, max_length))\n",
    "answers  = np.random.randint(0, 1, size=(num_samples, answer_vocabulary_size))\n",
    "\n",
    "#model.fit([text, question], answers, epochs=10, batch_size=128)\n",
    "model.fit({'text': text, 'question': question},\n",
    "          answers,\n",
    "          epochs=10,\n",
    "          batch_size=128)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "posts (InputLayer)              (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, None, 50000)  12800000    posts[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, None, 128)    32000128    embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, None, 128)    0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, None, 256)    164096      max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, None, 256)    327936      conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, None, 256)    0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, None, 256)    327936      max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, None, 256)    327936      conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d (GlobalMax (None, 256)          0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 128)          32896       global_max_pooling1d[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "age (Dense)                     (None, 1)            129         dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "income (Dense)                  (None, 10)           1290        dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "gender (Dense)                  (None, 1)            129         dense_9[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 45,982,476\n",
      "Trainable params: 45,982,476\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n# feed the data\\n# model.fit(posts, [age_targets, income_targets, gender_targets], epochs=10, batch_size=64)\\nmodel.fit(posts, \\n          {'age': age_targets,\\n           'income': income_targets,\\n           'gender': gender_targets},\\n           epochs=10,\\n           batch_size=64)\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Multi-output models\n",
    "\n",
    "1. In the same way, you can use the functional API to build models with multiple outputs (or multiple heads). A simple example is a network\n",
    "   that attempts to simultaneously predict different properties of the data, such as a network that takes as input a series of social media\n",
    "   posts from a single anonymous person and tries to predict attributes of that person, such as age, gender, and income level.\n",
    "\n",
    "2. Importantly, training such a model requires the ability to specify different loss functions for different heads of the network: for\n",
    "   instance, age prediction is a scalar regression task, but gender prediction is a binary classification task, requiring a different\n",
    "   training procedure. But because gradient descent requires you to minimize a scalar, you must combine these losses into a single value\n",
    "   in order to train the model. The simplest way to combine different losses is to sum them all. In Keras, you can use either a list or\n",
    "   a dictionary of losses in compile to specify different objects for different outputs; the resulting loss values are summed into a global\n",
    "   loss, which is minimized during training.\n",
    "\n",
    "3. Note that very imbalanced loss contributions will cause the model representations to be optimized preferentially for the task with\n",
    "   the largest individual loss, at the expense of the other tasks. To remedy this, you can assign different levels of importance to the\n",
    "   loss values in their contribution to the final loss. This is useful in particular if the losses’ values use different scales.\n",
    "   For instance, the mean squared error (MSE) loss used for the age-regression task typically takes a value around 3–5, whereas the\n",
    "   crossentropy loss used for the gender-classification task can be as low as 0.1. In such a situation, to balance the contribution\n",
    "   of the different losses, you can assign a weight of 10 to the crossentropy loss and a weight of 0.25 to the MSE loss.\n",
    "\"\"\"\n",
    "vocabulary_size = 50000\n",
    "num_income_groups = 10\n",
    "\n",
    "posts_input = Input(shape=(None,), dtype='int32', name='posts')\n",
    "embedded_posts = layers.Embedding(256, vocabulary_size)(posts_input)\n",
    "x = layers.Conv1D(128, 5, activation='relu')(embedded_posts)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.Conv1D(256, 5, activation='relu')(x)\n",
    "x = layers.Conv1D(256, 5, activation='relu')(x)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.Conv1D(256, 5, activation='relu')(x)\n",
    "x = layers.Conv1D(256, 5, activation='relu')(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "\n",
    "age_prediction = layers.Dense(1, name='age')(x)\n",
    "income_prediction = layers.Dense(num_income_groups, activation='softmax', name='income')(x)\n",
    "gender_prediction = layers.Dense(1, activation='sigmoid', name='gender')(x)\n",
    "\n",
    "model = Model(posts_input, [age_prediction, income_prediction, gender_prediction])\n",
    "\n",
    "# multiple losses\n",
    "# model.compile(optimizer='rmsprop', loss=['mse', 'categorical_crossentropy', 'binary_crossentropy'])\n",
    "# model.compile(optimizer='rmsprop',\n",
    "#               loss={'age': 'mse',\n",
    "#                     'income': 'categorical_crossentropy',\n",
    "#                     'gender': 'binary_crossentropy'})\n",
    "\n",
    "# loss weight\n",
    "# model.compile(optimizer='rmsprop', loss={'age': 'mse', 'income': 'categorical_crossentropy', 'gender': 'binary_crossentropy'},\n",
    "#                                    loss_weights={'age': 0.25, 'income': 1., 'gender': 10.})\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss=['mse', 'categorical_crossentropy', 'binary_crossentropy'],\n",
    "              loss_weights=[0.25, 1., 10.])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "\"\"\"\n",
    "# feed the data\n",
    "# model.fit(posts, [age_targets, income_targets, gender_targets], epochs=10, batch_size=64)\n",
    "model.fit(posts, \n",
    "          {'age': age_targets,\n",
    "           'income': income_targets,\n",
    "           'gender': gender_targets},\n",
    "           epochs=10,\n",
    "           batch_size=64)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Directed acyclic graphs of layers (1)\n",
    "\n",
    "1. With the functional API, not only can you build models with multiple inputs and multiple outputs, but you can also implement networks\n",
    "   with a complex internal topology. Neural networks in Keras are allowed to be arbitrary directed acyclic graphs of layers. The qualifier\n",
    "   acyclic is important: these graphs can’t have cycles. It’s impossible for a tensor x to become the input of one of the layers that\n",
    "   generated x. The only processing loops that are allowed (that is, recurrent connections) are those internal to recurrent layers.\n",
    "\n",
    "2. Several common neural-network components are implemented as graphs. Two notable ones are Inception modules and residual connections.\n",
    "\n",
    "3. The purpose of 1 × 1 convolutions: The convolutions extract spatial patches around every tile in an input tensor and apply the same\n",
    "   transformation to each patch. An edge case is when the patches extracted consist of a single tile. The convolution operation then\n",
    "   becomes equivalent to running each tile vector through a Dense layer: it will compute features that mix together information from\n",
    "   the channels of the input tensor, but it won’t mix information across space (because it’s looking at one tile at a time). Such\n",
    "   1 × 1 convolutions (also called pointwise convolutions) are featured in Inception modules, where they contribute to factoring out\n",
    "   channel-wise feature learning and spacewise feature learning—a reasonable thing to do if you assume that each channel is highly\n",
    "   autocorrelated across space, but different channels may not be highly correlated with each other.\n",
    "\"\"\"\n",
    "\n",
    "branch_a = layers.Conv2D(128, 1, activation='relu', strides=2)(x)\n",
    "\n",
    "branch_b = layers.Conv2D(128, 1, activation='relu')(x)\n",
    "branch_b = layers.Conv2D(128, 3, activation='relu', strides=2)(branch_b)\n",
    "\n",
    "branch_c = layers.AveragePooling2D(3, strides=2)(x)\n",
    "branch_c = layers.Conv2D(128, 3, activation='relu')(branch_c)\n",
    "\n",
    "branch_d = layers.Conv2D(128, 1, activation='relu')(x)\n",
    "branch_d = layers.Conv2D(128, 3, activation='relu')(branch_d)\n",
    "branch_d = layers.Conv2D(128, 3, activation='relu', strides=2)(branch_d)\n",
    "\n",
    "# concatenate the branch outputs to obtain the module output\n",
    "output = layers.concatenate([branch_a, branch_b, branch_c, branch_d], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Directed acyclic graphs of layers (2)\n",
    "\n",
    "1. A residual connection consists of making the output of an earlier layer available as input to a later layer, effectively creating\n",
    "   a shortcut in a sequential network. Rather than being concatenated to the later activation, the earlier output is summed with the\n",
    "   later activation, which assumes that both activations are the same size. If they’re different sizes, you can use a linear transformation\n",
    "   to reshape the earlier activation into the target shape.\n",
    "\n",
    "2. Representational bottlenecks in deep learning:\n",
    "   In a Sequential model, each successive representation layer is built on top of the previous one, which means it only has access to\n",
    "   information contained in the activation of the previous layer. If one layer is too small (for example, it has features that are too\n",
    "   low-dimensional), then the model will be constrained by how much information can be crammed into the activations of this layer.\n",
    "   You can grasp this concept with a signal-processing analogy: if you have an audio processing pipeline that consists of a series of\n",
    "   operations, each of which takes as input the output of the previous operation, then if one operation crops your signal to a low-frequency\n",
    "   range (for example, 0–15 kHz), the operations downstream will never be able to recover the dropped frequencies. Any loss of information\n",
    "   is permanent. Residual connections, by reinjecting earlier information downstream, partially solve this issue for deep-learning models.\n",
    "\n",
    "3. Vanishing gradients in deep learning:\n",
    "   Backpropagation, the master algorithm used to train deep neural networks, works by propagating a feedback signal from the output loss\n",
    "   down to earlier layers. If this feedback signal has to be propagated through a deep stack of layers, the signal may become tenuous or\n",
    "   even be lost entirely, rendering the network untrainable. This issue is known as vanishing gradients. This problem occurs both with\n",
    "   deep networks and with recurrent networks over very long sequences—in both cases, a feedback signal must be propagated through a\n",
    "   long series of operations. You’re already familiar with the solution that the LSTM layer uses to address this problem in recurrent\n",
    "   networks: it introduces a carry track that propagates information parallel to the main processing track. Residual connections work in\n",
    "   a similar way in feedforward deep networks, but they’re even simpler: they introduce a purely linear information carry track parallel\n",
    "   to the main layer stack, thus helping to propagate gradients through arbitrarily deep stacks of layers. \n",
    "\"\"\"\n",
    "\n",
    "# add\n",
    "y = layers.Conv2D(128, 3, activation='relu', padding='same')(x)\n",
    "y = layers.Conv2D(128, 3, activation='relu', padding='same')(y)\n",
    "y = layers.Conv2D(128, 3, activation='relu', padding='same')(y)\n",
    "y = layers.add([y, x])\n",
    "\n",
    "# transform\n",
    "y = layers.Conv2D(128, 3, activation='relu', padding='same')(x)\n",
    "y = layers.Conv2D(128, 3, activation='relu', padding='same')(y)\n",
    "y = layers.MaxPooling2D(2, strides=2)(y)\n",
    "\n",
    "residual = layers.Conv2D(128, 1, strides=2, padding='same')(x)\n",
    "y = layers.add([y, residual])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, None, 128)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, None, 128)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_7 (LSTM)                   (None, 32)           20608       input_4[0][0]                    \n",
      "                                                                 input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 64)           0           lstm_7[0][0]                     \n",
      "                                                                 lstm_7[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 1)            65          concatenate_4[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 20,673\n",
      "Trainable params: 20,673\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Layer weight sharing\n",
    "\n",
    "One more important feature of the functional API is the ability to reuse a layer instance several times. When you call a layer instance\n",
    "twice, instead of instantiating a new layer for each call, you reuse the same weights with every call. This allows you to build models\n",
    "that have shared branches — several branches that all share the same knowledge and perform the same operations. That is, they share\n",
    "the same representations and learn these representations simultaneously for different sets of inputs.\n",
    "\"\"\"\n",
    "\n",
    "# Instantiates a single LSTM layer, once\n",
    "lstm = layers.LSTM(32)\n",
    "\n",
    "# Building the left branch of the model\n",
    "left_input = Input(shape=(None, 128))\n",
    "left_output = lstm(left_input)\n",
    "\n",
    "# Building the right branch of the model\n",
    "right_input = Input(shape=(None, 128))\n",
    "right_output = lstm(right_input)\n",
    "\n",
    "# Builds the classifier on top\n",
    "merged = layers.concatenate([left_output, right_output], axis=-1)\n",
    "predictions = layers.Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "# model\n",
    "model = Model([left_input, right_input], predictions)\n",
    "#model.fit([left_data, right_data], targets)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Models as layers\n",
    "\n",
    "1. Importantly, in the functional API, models can be used as you’d use layers—effectively, you can think of a model as a “bigger layer.”\n",
    "   This is true of both the Sequential and Model classes.\n",
    "\n",
    "2. When you call a model instance, you’re reusing the weights of the model—exactly like what happens when you call a layer instance.\n",
    "   Calling an instance, whether it’s a layer instance or a model instance, will always reuse the existing learned representations of the\n",
    "   instance—which is intuitive.\n",
    "\n",
    "3.  One simple practical example of what you can build by reusing a model instance is a vision model that uses a dual camera as its input:\n",
    "    two parallel cameras, a few centimeters (one inch) apart. Such a model can perceive depth, which can be useful in many applications.\n",
    "    You shouldn’t need two independent models to extract visual features from the left camera and the right camera before merging\n",
    "    the two feeds. Such low-level processing can be shared across the two inputs: that is, done via layers that use the same weights\n",
    "    and thus share the same representations. \n",
    "\"\"\"\n",
    "from keras import layers\n",
    "from keras import applications\n",
    "from keras import Input\n",
    "\n",
    "xception_base = applications.Xception(weights=None, include_top=False)\n",
    "\n",
    "left_input = Input(shape=(250, 250, 3))\n",
    "left_features = xception_base(left_input)\n",
    "\n",
    "right_input = Input(shape=(250, 250, 3))\n",
    "right_input = xception_base(right_input)\n",
    "\n",
    "merged_features = layers.concatenate([left_features, right_input], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
